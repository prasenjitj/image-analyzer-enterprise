# Enterprise Image Analyzer - Production Ready

> **ğŸ“‹ Status Update - Repository Restructuring Complete âœ…**  
> All issues from the recent repository restructuring have been identified and fixed:
> - âœ… Server startup path issues resolved  
> - âœ… Missing directories auto-created
> - âœ… Security vulnerabilities patched
> - âœ… All functionality tested and verified working
> 
> **ğŸš€ Ready to Use**: `python3 server/run_server.py` now starts successfully!  
> See [UPDATED_QUICK_START_GUIDE.md](UPDATED_QUICK_START_GUIDE.md) for details.

A scalable, production-ready image analysis system capable of processing millions of images with advanced queuing, batch management, and real-time monitoring. Uses external AI API endpoints for image analysis instead of local OCR processing.

## ğŸš€ Features

- **Massive Scale Processing**: Handle millions of image URLs efficiently  
- **Intelligent Batching**: Process images in configurable chunks with progress tracking
- **PostgreSQL + Redis**: Persistent storage with high-performance caching
- **Background Workers**: Parallel processing with automatic retry logic
- **Real-time Dashboard**: Live progress tracking and system metrics
- **Multi-format Export**: CSV, JSON, Excel exports with filtering
- **RESTful API**: Complete programmatic access to all features
- **Fault Tolerance**: Automatic error recovery and pause/resume support
- **External AI Integration**: Uses configurable API endpoints for image analysis

## ğŸ“‹ Quick Start (5 minutes)

For full, step-by-step setup with screenshots and troubleshooting, see QUICK_START.md.

### Prerequisites
- Python 3.8+
- PostgreSQL 12+
- Redis 6+
- External AI API endpoint (configured via environment variables)

### Environment Variables

Key environment variables (full list in [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md#required-environment-variables)):

```bash
# External AI API Configuration
API_ENDPOINT_URL=http://your-api-endpoint:8000/generate  # External AI API endpoint

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/image_analyzer

# Redis
REDIS_URL=redis://localhost:6379/0

# Application
SECRET_KEY=your_secret_key_here
MAX_UPLOAD_SIZE=104857600               # 100MB in bytes
REQUEST_TIMEOUT=60                      # API request timeout in seconds
```

### Setup
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure environment
cp .env.example .env
# Edit .env with your PostgreSQL, Redis, and API credentials

# 3. Initialize database
python setup.py --init-db

# 4. Verify setup
python setup.py --health-check

# 5. Start the application (Local Development)
python3 server/run_server.py

# For production/container deployment
python3 server/run_server_cloud.py

# In another terminal: Start background workers (optional - auto-started)
python server/run_workers.py
```

**Access Dashboard**: http://localhost:5001

## ğŸ“– Documentation

| Document | Purpose |
|----------|---------|
| **[QUICK_START.md](docs/QUICK_START.md)** | Detailed setup and first batch guide |
| **[API_DOCUMENTATION.md](docs/API_DOCUMENTATION.md)** | Complete API reference |
| **[DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md)** | Production deployment instructions |
| **[GCP_DEPLOYMENT_GUIDE.md](docs/GCP_DEPLOYMENT_GUIDE.md)** | Google Cloud Platform deployment guide |
| **[PERFORMANCE_OPTIMIZATION_GUIDE.md](docs/PERFORMANCE_OPTIMIZATION_GUIDE.md)** | Performance optimization and scaling strategies |
| **[GCP_FREE_DEPLOYMENT_GUIDE.md](docs/GCP_FREE_DEPLOYMENT_GUIDE.md)** | Free tier GCP deployment guide |

## ğŸš€ Deployment Scripts

For quick GCP deployment, use the provided bash scripts:

```bash
# Complete automated deployment
./scripts/deploy_all.sh [VM_IP] [PROJECT_ID] [REGION]

# Or run individual steps:
./scripts/create_env_yaml.sh [VM_IP] [PROJECT_ID]
./scripts/create_dockerfiles.sh
./scripts/setup_vpc_connector.sh [PROJECT_ID] [REGION]
./scripts/build_and_deploy_server.sh [PROJECT_ID] [REGION]
./scripts/build_and_deploy_worker.sh [PROJECT_ID] [REGION]
```

## ğŸ¯ Common Tasks

### Upload and Process Images
```bash
# Via Web UI
1. Open http://localhost:5001
2. Drag & drop CSV file with URLs
3. Click "Create Batch"

# Via API
curl -X POST http://localhost:5001/api/v1/batches \
  -F "file=@urls.csv" \
  -F "batch_name=My Batch"
```

### Monitor Progress
```bash
# Web dashboard: http://localhost:5001
# Or via API:
curl http://localhost:5001/api/v1/batches/{batch_id}/status
```

### Export Results
```bash
# Web UI: Click Export dropdown on batch detail page
# Or via API:
curl -O "http://localhost:5001/api/v1/batch-data/export?format=csv"
```

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ enterprise_app.py          # Flask web server
â”œâ”€â”€ background_worker.py       # Batch processing workers
â”œâ”€â”€ batch_manager.py           # Batch orchestration
â”œâ”€â”€ job_queue.py              # Redis queue management
â”œâ”€â”€ export_api.py             # Export functionality
â”œâ”€â”€ polling_api.py            # Real-time status API
â”œâ”€â”€ database_models.py        # PostgreSQL schema
â”œâ”€â”€ export_manager.py         # Export formats
â”œâ”€â”€ cache.py                  # Caching layer
â”œâ”€â”€ processor.py              # External API-based image processing
â””â”€â”€ enterprise_config.py      # Configuration management

server/
â”œâ”€â”€ run_server.py             # Local development server
â”œâ”€â”€ run_server_cloud.py       # Container/Cloud Run server
â”œâ”€â”€ run_workers.py            # Background worker launcher
â”œâ”€â”€ run_worker_cloud.py       # Cloud worker launcher
â””â”€â”€ run_worker_http.py        # HTTP worker service

scripts/
â”œâ”€â”€ deploy_all.sh             # Complete deployment automation
â”œâ”€â”€ build_and_deploy_server.sh # Server deployment
â”œâ”€â”€ build_and_deploy_worker.sh # Worker deployment
â”œâ”€â”€ create_env_yaml.sh        # Environment configuration
â””â”€â”€ setup_vpc_connector.sh    # VPC network setup

templates/
â”œâ”€â”€ modern_enterprise_dashboard.html  # Main UI
â”œâ”€â”€ modern_admin_dashboard.html       # Admin panel
â”œâ”€â”€ modern_system_status.html         # System metrics
â””â”€â”€ shared-ui-components.css          # Shared styles
```

## âš¡ Performance (API-Based Processing)

**Processing Rates** (based on external API performance):
- **Standard Configuration**: ~100-200 URLs/minute (depends on external API response times)
- **Optimized Setup**: ~300+ URLs/minute with proper caching and error handling
- **Concurrent Workers**: Configurable background workers for parallel processing
- **Memory**: ~200MB base + optimized workers with memory monitoring

Performance depends on the external API endpoint response times and network conditions. The system includes automatic retry logic and timeout handling for reliability.

## ğŸ’° Cost Estimation

### Cost Assumptions & Methodology

**Processing Rates** (based on external API performance):
- **Standard Configuration**: ~100-200 URLs/minute
- **Optimized Setup**: ~300+ URLs/minute with proper caching

**Cost Components**:
- **External API Costs**: Depends on your API provider pricing
- **Infrastructure**: PostgreSQL, Redis, and compute resources
- **Storage**: Database storage for results and caching

**Assumptions**:
- External API costs vary by provider
- Infrastructure costs based on cloud provider pricing
- 70% success rate (30% retries due to network/API issues)

### Cost Optimization Strategies

**Reduce API Costs**:
- Implement result caching (reduce duplicate API calls)
- Batch requests efficiently to minimize retries
- Monitor API usage and optimize for cost efficiency
- Use appropriate timeout and retry settings

**Infrastructure Costs**:
- **Development/Testing**: Minimal costs using free tier resources
- **Small Production**: <$50/month for moderate usage
- **Enterprise**: Scale based on processing volume and storage needs

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/

# Run specific test
python -m pytest tests/test_enterprise_system.py -v

# With coverage
python -m pytest tests/ --cov=src
```

## ğŸ”§ Troubleshooting

**Database Connection Error**
```bash
psql -h localhost -U postgres -d imageprocessing
# Should connect without errors
```

**Redis Connection Error**
```bash
redis-cli ping
# Should return: PONG
```

**Check System Health**
```bash
python setup.py --health-check
```

For more help, see [QUICK_START.md](QUICK_START.md) troubleshooting section.

## ğŸ“ Support

- **System Status**: http://localhost:5001/system/status
- **Health Check**: `python setup.py --health-check`
- **Logs**: `logs/enterprise_app.log`
- **API Docs**: [API_DOCUMENTATION.md](API_DOCUMENTATION.md)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
